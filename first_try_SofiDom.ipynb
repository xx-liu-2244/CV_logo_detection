{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGO DETECTION PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compulsory logos: Adidas, Nike, Puma, The North Face, Under Armour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.14.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 27.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.14.0 typeguard-2.13.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/anaconda/envs/py38_default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "bboxes = []\n",
    "imagePaths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training images: 40519\n",
      "total noise images: 2979\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"images\"\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "noise_dir = os.path.join(base_dir, 'noise')\n",
    "print('total training images:', len(os.listdir(train_dir)))\n",
    "print('total noise images:', len(os.listdir(noise_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46163\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"annot_train.csv\")\n",
    "data.head()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DLCV_logo_project/train/london_1682860930385563294_20180101.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ef876ae62200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mimagePaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m608\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m608\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   \"\"\"\n\u001b[0;32m--> 295\u001b[0;31m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[1;32m    296\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DLCV_logo_project/train/london_1682860930385563294_20180101.jpg'"
     ]
    }
   ],
   "source": [
    "file = \"annot_train_scaled.csv\"\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    if header != None:\n",
    "        for row in reader:\n",
    "            labels.append(row[3])\n",
    "            bboxes.append((np.float32(row[4]),np.float32(row[5]),np.float32(row[6]),np.float32(row[7])))  #ALREADY SCALED BETWEEN 0 AND 1\n",
    "            imagePath = os.path.join(train_dir,row[0])\n",
    "            imagePaths.append(imagePath)\n",
    "            image = cv2.imread(imagePath)\n",
    "            image = load_img(imagePath, target_size=(608, 608))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"annot_noise_scaled.csv\"\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    if header != None:\n",
    "        for row in reader:\n",
    "            labels.append(row[3])\n",
    "            bboxes.append((row[4],row[5],row[6],row[7]))  #ALREADY SCALED BETWEEN 0 AND 1\n",
    "            imagePath = os.path.join(noise_dir,row[0])\n",
    "            image = cv2.imread(imagePath)\n",
    "            image = load_img(imagePath, target_size=(608, 608))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.startfile(imagePaths[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608, 608, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adidas'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5296875, 0.615625, 0.5953125, 0.6859375)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels),len(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype=\"float32\") / 255.0\n",
    "labels = np.array(labels)\n",
    "bboxes = np.array(bboxes, dtype=\"float32\")\n",
    "imagePaths = np.array(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels),len(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = train_test_split(data, labels, bboxes, imagePaths,\n",
    "test_size=0.2, random_state=42)\n",
    "\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainLabels, testLabels) = split[2:4]\n",
    "(trainBBoxes, testBBoxes) = split[4:6]\n",
    "(trainPaths, testPaths) = split[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainBBoxes),len(testPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = np.array(trainImages)\n",
    "testImages = np.array(testImages)\n",
    "trainLabels = np.array(trainLabels)\n",
    "testLabels = np.array(testLabels)\n",
    "trainBBoxes = np.array(trainBBoxes)\n",
    "testBBoxes = np.array(testBBoxes)\n",
    "trainPaths = np.array(trainPaths)\n",
    "testPaths = np.array(testPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(608, 608, 3)))\n",
    "\n",
    "# freeze all VGG layers \n",
    "vgg.trainable = False\n",
    "\n",
    "# flatten the max-pooling output of VGG\n",
    "flatten = vgg.output\n",
    "flatten = Flatten()(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a fully-connected layer header to output the predicted\n",
    "# bounding box coordinates\n",
    "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
    "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
    "coordinates = Dense(4, activation=\"sigmoid\",name=\"bounding_box\")(bboxHead)\n",
    "\n",
    "# construct a second fully-connected layer head, this one to predict\n",
    "# the class label\n",
    "softmaxHead = Dense(512, activation=\"relu\")(flatten)\n",
    "softmaxHead = Dropout(0.5)(softmaxHead)\n",
    "softmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\n",
    "softmaxHead = Dropout(0.5)(softmaxHead)\n",
    "probclass = Dense(5, activation=\"softmax\",name=\"class_label\")(softmaxHead)\n",
    "\n",
    "# put together our model which accept an input image and then output\n",
    "# bounding box coordinates and a class label\n",
    "model = Model(inputs=vgg.input,outputs=(coordinates, probclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE IOU LOSS\n",
    "\n",
    "@tf.function  \n",
    "def IoUloss(y_true,y_pred):\n",
    "    xA = max(y_true[0], y_pred[0])       \n",
    "    yA = max(y_true[1], y_pred[1])\n",
    "    xB = min(y_true[2], y_pred[2])\n",
    "    yB = min(y_true[3], y_pred[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxtrueArea = (y_true[2] - y_true[0]) * (y_true[3] - y_true[1])    \n",
    "    boxpredArea = (y_pred[2] - y_pred[0]) * (y_pred[3] - y_pred[1])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxtrueArea + boxpredArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.14285715>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoUloss((0,0.2,0.4,0.6),(0.2,0,0.6,0.4))  #intersection over unit of two squares of 4x4 overlapping at the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tfa.losses.GIoULoss()\n",
    "\n",
    "categorical_crossentropy= tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 608, 608, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 608, 608, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 608, 608, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 304, 304, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 304, 304, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 304, 304, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 152, 152, 128 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 152, 152, 256 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 152, 152, 256 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 152, 152, 256 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 76, 76, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 76, 76, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 76, 76, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 76, 76, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 38, 38, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 38, 38, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 38, 38, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 38, 38, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 19, 19, 512)  0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 184832)       0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          94634496    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          23658624    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bounding_box (Dense)            (None, 4)            132         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "class_label (Dense)             (None, 5)            2565        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 133,283,497\n",
      "Trainable params: 118,568,809\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define a dictionary to set the loss methods -- categorical\n",
    "# cross-entropy for the class label head and mean absolute error\n",
    "# for the bounding box head\n",
    "\n",
    "\n",
    "\n",
    "losses = {\n",
    "    \"class_label\": categorical_crossentropy,\n",
    "    \"bounding_box\": g1,\n",
    "}\n",
    "\n",
    "# define a dictionary that specifies the weights per loss (both the\n",
    "# class label and bounding box outputs will receive equal weight)\n",
    "lossWeights = {\n",
    "    \"class_label\": 1.0,\n",
    "    \"bounding_box\": 1.0\n",
    "}\n",
    "\n",
    "# initialize the optimizer, compile the model, and show the model\n",
    "INIT_LR = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 15\n",
    "\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "model.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=lossWeights)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTargets = {\"class_label\": trainLabels,\"bounding_box\": trainBBoxes}\n",
    "testTargets = {\"class_label\": testLabels,\"bounding_box\": testBBoxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 108s 1s/step - loss: 5.4243 - bounding_box_loss: 1.1818 - class_label_loss: 4.2425 - bounding_box_accuracy: 0.6000 - class_label_accuracy: 0.1833 - val_loss: 2.8378 - val_bounding_box_loss: 0.9715 - val_class_label_loss: 1.8663 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2333\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 4.2368 - bounding_box_loss: 0.9861 - class_label_loss: 3.2508 - bounding_box_accuracy: 0.6000 - class_label_accuracy: 0.2583 - val_loss: 2.9340 - val_bounding_box_loss: 0.9719 - val_class_label_loss: 1.9621 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2000\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 3.8923 - bounding_box_loss: 0.9860 - class_label_loss: 2.9062 - bounding_box_accuracy: 0.6000 - class_label_accuracy: 0.2833 - val_loss: 2.8006 - val_bounding_box_loss: 0.9719 - val_class_label_loss: 1.8287 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2000\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.7708 - bounding_box_loss: 0.9856 - class_label_loss: 1.7852 - bounding_box_accuracy: 0.6000 - class_label_accuracy: 0.4417 - val_loss: 2.6498 - val_bounding_box_loss: 0.9714 - val_class_label_loss: 1.6783 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.3000\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.2235 - bounding_box_loss: 0.9877 - class_label_loss: 1.2359 - bounding_box_accuracy: 0.6000 - class_label_accuracy: 0.5333 - val_loss: 2.5976 - val_bounding_box_loss: 0.9712 - val_class_label_loss: 1.6264 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.1333\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.1733 - bounding_box_loss: 0.9857 - class_label_loss: 1.1876 - bounding_box_accuracy: 0.6000 - class_label_accuracy: 0.5083 - val_loss: 2.5671 - val_bounding_box_loss: 0.9715 - val_class_label_loss: 1.5956 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2667\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.1779 - bounding_box_loss: 0.9858 - class_label_loss: 1.1922 - bounding_box_accuracy: 0.6083 - class_label_accuracy: 0.5167 - val_loss: 2.5223 - val_bounding_box_loss: 0.9713 - val_class_label_loss: 1.5510 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2667\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.0153 - bounding_box_loss: 0.9852 - class_label_loss: 1.0301 - bounding_box_accuracy: 0.6083 - class_label_accuracy: 0.5583 - val_loss: 2.5594 - val_bounding_box_loss: 0.9719 - val_class_label_loss: 1.5875 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2333\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.8044 - bounding_box_loss: 0.9845 - class_label_loss: 0.8199 - bounding_box_accuracy: 0.6083 - class_label_accuracy: 0.7167 - val_loss: 2.7149 - val_bounding_box_loss: 1.0323 - val_class_label_loss: 1.6826 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.3000\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.7756 - bounding_box_loss: 0.9853 - class_label_loss: 0.7903 - bounding_box_accuracy: 0.6083 - class_label_accuracy: 0.6917 - val_loss: 2.6357 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.6637 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2333\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.7585 - bounding_box_loss: 0.9842 - class_label_loss: 0.7743 - bounding_box_accuracy: 0.6250 - class_label_accuracy: 0.7250 - val_loss: 2.7271 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.7550 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2667\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.5114 - bounding_box_loss: 0.9862 - class_label_loss: 0.5252 - bounding_box_accuracy: 0.6500 - class_label_accuracy: 0.8083 - val_loss: 2.6623 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.6903 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.2333\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.6380 - bounding_box_loss: 0.9862 - class_label_loss: 0.6518 - bounding_box_accuracy: 0.6000 - class_label_accuracy: 0.7500 - val_loss: 2.6074 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.6354 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.3000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.5765 - bounding_box_loss: 0.9862 - class_label_loss: 0.5903 - bounding_box_accuracy: 0.6250 - class_label_accuracy: 0.7833 - val_loss: 2.5385 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.5664 - val_bounding_box_accuracy: 0.5667 - val_class_label_accuracy: 0.3000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.7140 - bounding_box_loss: 0.9862 - class_label_loss: 0.7278 - bounding_box_accuracy: 0.6083 - class_label_accuracy: 0.6833 - val_loss: 2.6029 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.6309 - val_bounding_box_accuracy: 0.5333 - val_class_label_accuracy: 0.3000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.5986 - bounding_box_loss: 0.9862 - class_label_loss: 0.6124 - bounding_box_accuracy: 0.6083 - class_label_accuracy: 0.7667 - val_loss: 2.5975 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.6254 - val_bounding_box_accuracy: 0.5333 - val_class_label_accuracy: 0.2333\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.5265 - bounding_box_loss: 0.9862 - class_label_loss: 0.5403 - bounding_box_accuracy: 0.5917 - class_label_accuracy: 0.7833 - val_loss: 2.6836 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.7115 - val_bounding_box_accuracy: 0.5333 - val_class_label_accuracy: 0.2333\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.4314 - bounding_box_loss: 0.9862 - class_label_loss: 0.4453 - bounding_box_accuracy: 0.5667 - class_label_accuracy: 0.8250 - val_loss: 2.5232 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.5512 - val_bounding_box_accuracy: 0.5333 - val_class_label_accuracy: 0.2667\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.3253 - bounding_box_loss: 0.9862 - class_label_loss: 0.3392 - bounding_box_accuracy: 0.5750 - class_label_accuracy: 0.8833 - val_loss: 2.6215 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.6494 - val_bounding_box_accuracy: 0.5333 - val_class_label_accuracy: 0.2333\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.3288 - bounding_box_loss: 0.9862 - class_label_loss: 0.3426 - bounding_box_accuracy: 0.5750 - class_label_accuracy: 0.9000 - val_loss: 2.6116 - val_bounding_box_loss: 0.9720 - val_class_label_loss: 1.6395 - val_bounding_box_accuracy: 0.5333 - val_class_label_accuracy: 0.2667\n",
      "[INFO] saving object detector model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "H = model.fit(\n",
    "    trainImages, trainTargets,\n",
    "    validation_data=(testImages, testTargets),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1)\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "os.makedirs('models/') \n",
    "model.save(\"models/first.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
